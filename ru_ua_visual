"""
Russo-Ukrainian War Knowledge Graph: Overlap & Visualisation Suite

This script compares the QIDs harvested via:
  - Method A: Wikipedia link traversal (ru_ua_wiki_link_harvest.py)
  - Method B: Wikidata SPARQL harvest (ru_ua_wikidata_harvest.py)

It reports:
  * global overlap statistics between the two QID sets
  * stratified overlap per attribution bucket: {Russia, Ukraine, Other}
  * multiple visualisations:
      - global overlap bar chart
      - per-country coverage bar chart (Wiki vs Wikidata)
      - per-country overlap composition (Wiki-only / Both / WD-only)
      - per-country Jaccard similarity
      - optional Venn diagrams per country

Run this AFTER you have already produced the JSONL outputs from both harvest scripts.
"""

import json
import os
import re
from typing import Dict, Set, Optional

import numpy as np
import matplotlib.pyplot as plt
from matplotlib_venn import venn2

# ---------------------------------------------------------------------------
# Configuration: adjust these paths to your actual JSONL outputs
# ---------------------------------------------------------------------------

# Example:
#   FILE_WIKI_HARVEST     = "data/entities_from_links.jsonl"
#   FILE_WIKIDATA_HARVEST = "data/ru_ua_keywords.jsonl"

FILE_WIKI_HARVEST = "entities_from_links.jsonl"        # Wikipedia link-based entities
FILE_WIKIDATA_HARVEST = "old/ru_ua_keywords.jsonl"  # Wikidata SPARQL-based entities
OUTPUT_DIR = "overlap_analysis_output"

# ---------------------------------------------------------------------------
# Country & attribution normalization
# ---------------------------------------------------------------------------

# Direct label normalization (for fields like "attribution", "inferred_country", etc.)
COUNTRY_LABEL_MAP = {
    "russia": "Russia",
    "russian federation": "Russia",
    "россия": "Russia",
    "российская федерация": "Russia",
    "ukraine": "Ukraine",
    "ukrainian": "Ukraine",
    "украина": "Ukraine",
    "american": "Other",
    "usa": "Other",
    "u.s.a.": "Other",
    "united states": "Other",
    "other": "Other",
    "unknown": "Other",
}

# Structured QID -> attribution (if we see these QIDs in claims / geo props)
STRUCTURED_COUNTRY_QIDS = {
    "Q159": "Russia",   # Russia
    "Q212": "Ukraine",  # Ukraine
}

# Keyword-based fallback on descriptions (en / ru / uk)
KEYWORD_PATTERNS = {
    "Russia": re.compile(r"\brussian\b|\bроссийск|\bроссия\b", re.IGNORECASE),
    "Ukraine": re.compile(r"\bukrainian\b|\bukraine\b|\bукраин", re.IGNORECASE),
    # "Other" is the default, so we do not define a pattern for it.
}


# ---------------------------------------------------------------------------
# Utility functions
# ---------------------------------------------------------------------------

def ensure_dir(path: str) -> None:
    """Create directory if it does not exist."""
    if not os.path.exists(path):
        os.makedirs(path)


def normalize_qid(raw: Optional[str]) -> Optional[str]:
    """
    Normalize any form of Wikidata identifier / URI into a bare QID like "Q123".

    Handles:
      - "Q123"
      - "wd:Q123"
      - "http://www.wikidata.org/entity/Q123"
      - "https://www.wikidata.org/wiki/Q123?foo=bar"
    """
    if not raw:
        return None
    s = str(raw).strip()

    # Strip fragment / query
    s = s.split("#", 1)[0]
    s = s.split("?", 1)[0]

    # Strip prefix like "wd:" or URL path
    if s.startswith("wd:"):
        s = s[3:]
    if "/" in s:
        s = s.rsplit("/", 1)[-1]

    # Very defensive: only accept typical QID pattern
    if not s or not s.startswith("Q"):
        return None
    return s


def normalize_attribution_label(label: Optional[str]) -> str:
    """
    Map free-form attribution labels from previous pipeline(s) to {Russia, Ukraine, Other}.
    """
    if not label:
        return "Other"
    key = label.strip().lower()
    return COUNTRY_LABEL_MAP.get(key, "Other")


def infer_country_from_record(rec: dict) -> str:
    """
    Try to infer {Russia, Ukraine, Other} for a single record.

    Evidence order:
      1. Explicit attribution labels (attribution / inferred_country / country / side / allegiance)
      2. Structured claims (P17/P27/etc. mapped to Q159/Q212)
      3. Keyword match in multilingual descriptions (en/ru/uk)
      4. Fallback -> Other
    """
    # 1) Direct labels from previous pipeline (e.g. wiki_link_harvest)
    for key in ("attribution", "inferred_country", "country", "side", "allegiance"):
        if key in rec and rec[key]:
            return normalize_attribution_label(rec[key])

    # 2) Structured claims (if present, mostly for Wikidata harvest output)
    claims = rec.get("claims") or {}
    if isinstance(claims, dict):
        candidate_qids: Set[str] = set()
        # Standard geo / citizenship properties used in the project
        for prop in ("P17", "P27", "P495", "P131", "P276", "P159"):
            vals = claims.get(prop)
            if not vals:
                continue
            # We accept either a list of QIDs or a single QID string
            if isinstance(vals, (list, tuple)):
                for v in vals:
                    q = normalize_qid(v)
                    if q:
                        candidate_qids.add(q)
            else:
                q = normalize_qid(vals)
                if q:
                    candidate_qids.add(q)

        for q in candidate_qids:
            if q in STRUCTURED_COUNTRY_QIDS:
                return STRUCTURED_COUNTRY_QIDS[q]

    # 3) Fallback: regex on descriptions (en/ru/uk)
    desc_texts = []

    descs = rec.get("descriptions") or {}
    if isinstance(descs, dict):
        for lang in ("en", "ru", "uk"):
            val = descs.get(lang)
            if isinstance(val, str):
                desc_texts.append(val)

    # Some variants put a single "description" or "desc" at top-level
    for key in ("description", "desc"):
        if key in rec and isinstance(rec[key], str):
            desc_texts.append(rec[key])

    blob = " ".join(desc_texts)
    if blob:
        for label, pattern in KEYWORD_PATTERNS.items():
            if pattern.search(blob):
                return label

    # 4) Default
    return "Other"


def load_dataset(filepath: str, source_name: str) -> Dict[str, str]:
    """
    Load a JSONL file into a dict: { QID -> normalized_country_label }.

    It is intentionally robust to slightly different JSON structures produced in
    Meeting 1 vs Meeting 2:
      * 'qid' vs 'id' vs 'uri' vs 'item'
      * with or without 'attribution'
      * with or without 'claims'
    """
    data: Dict[str, str] = {}
    print(f"[{source_name}] Loading from {filepath} ...")

    if not os.path.exists(filepath):
        print(f"[{source_name}] WARNING: file not found, returning empty dataset.")
        return data

    n_lines = 0
    n_records = 0

    with open(filepath, "r", encoding="utf-8") as f:
        for line in f:
            n_lines += 1
            line = line.strip()
            if not line:
                continue
            try:
                rec = json.loads(line)
            except json.JSONDecodeError:
                # Skip malformed lines silently
                continue

            # Robust QID extraction
            qid = (
                normalize_qid(rec.get("qid"))
                or normalize_qid(rec.get("id"))
                or normalize_qid(rec.get("item"))
                or normalize_qid(rec.get("uri"))
            )
            if not qid:
                continue

            country = infer_country_from_record(rec)
            data[qid] = country
            n_records += 1

    print(f"[{source_name}] Parsed {n_records} valid records from {n_lines} lines.")
    return data


# ---------------------------------------------------------------------------
# Venn diagram (optional, but kept for completeness)
# ---------------------------------------------------------------------------

def generate_venn_diagram(
    set_a: Set[str],
    set_b: Set[str],
    category: str,
    label_a: str = "Wikipedia links",
    label_b: str = "Wikidata SPARQL",
) -> None:
    """
    Draw and save a simple 2-set Venn diagram for a given attribution category.
    """
    only_a = set_a - set_b
    only_b = set_b - set_a
    inter = set_a & set_b

    if not (only_a or only_b or inter):
        print(f"  -> [{category}] no data, skipping Venn diagram.")
        return

    plt.figure(figsize=(6, 6))
    v = venn2(
        subsets=(len(only_a), len(only_b), len(inter)),
        set_labels=(label_a, label_b),
    )

    # Some light styling for readability
    if v.get_patch_by_id("10"):
        v.get_patch_by_id("10").set_alpha(0.5)
    if v.get_patch_by_id("01"):
        v.get_patch_by_id("01").set_alpha(0.5)
    if v.get_patch_by_id("11"):
        v.get_patch_by_id("11").set_alpha(0.7)

    plt.title(f"QID Overlap by Attribution: {category}")

    filename = os.path.join(OUTPUT_DIR, f"venn_overlap_{category.lower()}.png")
    plt.tight_layout()
    plt.savefig(filename)
    plt.close()
    print(f"  -> [{category}] Venn diagram saved to {filename}")


# ---------------------------------------------------------------------------
# Additional visualisations
# ---------------------------------------------------------------------------

def plot_global_overlap_bar(wiki_qids: Set[str], wd_qids: Set[str]) -> None:
    """
    Plot a single bar chart showing:
      - QIDs only in Wikipedia link traversal
      - QIDs in both
      - QIDs only in Wikidata SPARQL
    """
    only_wiki = len(wiki_qids - wd_qids)
    only_wd = len(wd_qids - wiki_qids)
    inter = len(wiki_qids & wd_qids)

    labels = ["Wiki-only", "Both", "WD-only"]
    values = [only_wiki, inter, only_wd]

    plt.figure(figsize=(6, 4))
    x = np.arange(len(labels))
    plt.bar(x, values)
    plt.xticks(x, labels)
    plt.ylabel("Number of QIDs")
    plt.title("Global QID Overlap: Wikipedia links vs Wikidata SPARQL")

    for i, v in enumerate(values):
        plt.text(i, v + max(values) * 0.01, str(v), ha="center", va="bottom", fontsize=9)

    out_path = os.path.join(OUTPUT_DIR, "global_overlap_bar.png")
    plt.tight_layout()
    plt.savefig(out_path)
    plt.close()
    print(f"  -> Global overlap bar chart saved to {out_path}")


def plot_per_country_source_bar(stats: Dict[str, Dict[str, int]], categories: list) -> None:
    """
    For each attribution bucket (Russia / Ukraine / Other), plot a grouped bar chart
    comparing how many QIDs come from:
      - Wikipedia link traversal
      - Wikidata SPARQL
    """
    wiki_counts = [stats[cat]["wiki"] for cat in categories]
    wd_counts = [stats[cat]["wd"] for cat in categories]

    x = np.arange(len(categories))
    width = 0.35

    plt.figure(figsize=(7, 4))
    plt.bar(x - width / 2, wiki_counts, width, label="Wikipedia links")
    plt.bar(x + width / 2, wd_counts, width, label="Wikidata SPARQL")

    plt.xticks(x, categories)
    plt.ylabel("Number of QIDs")
    plt.title("Coverage per Attribution Bucket")
    plt.legend()

    out_path = os.path.join(OUTPUT_DIR, "per_country_source_coverage.png")
    plt.tight_layout()
    plt.savefig(out_path)
    plt.close()
    print(f"  -> Per-country source coverage chart saved to {out_path}")


def plot_per_country_overlap_composition(stats: Dict[str, Dict[str, int]], categories: list) -> None:
    """
    For each attribution bucket, plot a grouped bar chart showing:
      - QIDs only from Wikipedia links
      - QIDs present in both
      - QIDs only from Wikidata SPARQL

    This makes it easy to see where each source contributes uniquely.
    """
    wiki_only = [stats[cat]["wiki_only"] for cat in categories]
    both = [stats[cat]["inter"] for cat in categories]
    wd_only = [stats[cat]["wd_only"] for cat in categories]

    x = np.arange(len(categories))
    width = 0.25

    plt.figure(figsize=(8, 4))
    plt.bar(x - width, wiki_only, width, label="Wiki-only")
    plt.bar(x, both, width, label="Both")
    plt.bar(x + width, wd_only, width, label="WD-only")

    plt.xticks(x, categories)
    plt.ylabel("Number of QIDs")
    plt.title("Overlap Composition per Attribution Bucket")
    plt.legend()

    # Optional value labels
    max_val = max(wiki_only + both + wd_only) if (wiki_only + both + wd_only) else 0
    for i, (w, b, d) in enumerate(zip(wiki_only, both, wd_only)):
        if w:
            plt.text(x[i] - width, w + max_val * 0.01, str(w), ha="center", va="bottom", fontsize=8)
        if b:
            plt.text(x[i], b + max_val * 0.01, str(b), ha="center", va="bottom", fontsize=8)
        if d:
            plt.text(x[i] + width, d + max_val * 0.01, str(d), ha="center", va="bottom", fontsize=8)

    out_path = os.path.join(OUTPUT_DIR, "per_country_overlap_composition.png")
    plt.tight_layout()
    plt.savefig(out_path)
    plt.close()
    print(f"  -> Per-country overlap composition chart saved to {out_path}")


def plot_per_country_jaccard(stats: Dict[str, Dict[str, int]], categories: list) -> None:
    """
    Plot Jaccard similarity per attribution bucket:
        J(A, B) = |A ∩ B| / |A ∪ B|
    where A = QIDs from Wikipedia, B = QIDs from Wikidata.
    """
    jaccards = []
    for cat in categories:
        inter = stats[cat]["inter"]
        union = stats[cat]["wiki"] + stats[cat]["wd"] - inter
        if union > 0:
            jaccards.append(inter / union)
        else:
            jaccards.append(0.0)

    x = np.arange(len(categories))
    plt.figure(figsize=(6, 4))
    plt.bar(x, jaccards)
    plt.xticks(x, categories)
    plt.ylim(0, 1.0)
    plt.ylabel("Jaccard similarity")
    plt.title("Jaccard Similarity per Attribution Bucket")

    for i, v in enumerate(jaccards):
        plt.text(i, v + 0.02, f"{v:.2f}", ha="center", va="bottom", fontsize=9)

    out_path = os.path.join(OUTPUT_DIR, "per_country_jaccard.png")
    plt.tight_layout()
    plt.savefig(out_path)
    plt.close()
    print(f"  -> Per-country Jaccard chart saved to {out_path}")


# ---------------------------------------------------------------------------
# Main analysis function
# ---------------------------------------------------------------------------

def perform_analysis() -> None:
    ensure_dir(OUTPUT_DIR)

    # 1) Load both datasets
    wiki_data = load_dataset(FILE_WIKI_HARVEST, "Wikipedia Harvest")
    wd_data = load_dataset(FILE_WIKIDATA_HARVEST, "Wikidata Harvest")

    if not wiki_data or not wd_data:
        print("ERROR: one or both datasets are empty; cannot compute overlap.")
        return

    # 2) Global QID-level overlap
    wiki_qids = set(wiki_data.keys())
    wd_qids = set(wd_data.keys())
    inter = wiki_qids & wd_qids
    union = wiki_qids | wd_qids

    print("\n" + "=" * 60)
    print("GLOBAL QID OVERLAP: Wikipedia links vs Wikidata SPARQL")
    print("=" * 60)
    print(f"Total unique QIDs in Wikipedia set : {len(wiki_qids)}")
    print(f"Total unique QIDs in Wikidata set : {len(wd_qids)}")
    print(f"Intersection size                 : {len(inter)}")
    print(f"Only-in-Wikipedia                 : {len(wiki_qids - wd_qids)}")
    print(f"Only-in-Wikidata                  : {len(wd_qids - wiki_qids)}")
    if union:
        jaccard = len(inter) / len(union)
        print(f"Jaccard similarity (Wiki vs WD)   : {jaccard:.4f}")
    else:
        print("Jaccard similarity (Wiki vs WD)   : 0.0000")

    # 3) Stratified analysis by attribution bucket
    categories = ["Russia", "Ukraine", "Other"]
    stats: Dict[str, Dict[str, int]] = {}

    for cat in categories:
        print("\n" + "-" * 60)
        print(f"ATTRIBUTION BUCKET: {cat}")
        print("-" * 60)

        wiki_cat = {q for q, c in wiki_data.items() if c == cat}
        wd_cat = {q for q, c in wd_data.items() if c == cat}

        inter_cat = wiki_cat & wd_cat
        wiki_only_cat = wiki_cat - wd_cat
        wd_only_cat = wd_cat - wiki_cat

        print(f"  Wikipedia links ({cat}): {len(wiki_cat)}")
        print(f"  Wikidata SPARQL ({cat}): {len(wd_cat)}")
        print(f"  In both sets       ({cat}): {len(inter_cat)}")
        print(f"  Wiki-only          ({cat}): {len(wiki_only_cat)}")
        print(f"  WD-only            ({cat}): {len(wd_only_cat)}")

        # Store counts for later visualisations
        stats[cat] = {
            "wiki": len(wiki_cat),
            "wd": len(wd_cat),
            "inter": len(inter_cat),
            "wiki_only": len(wiki_only_cat),
            "wd_only": len(wd_only_cat),
        }

        # Write lists of QIDs for manual inspection
        details_path = os.path.join(OUTPUT_DIR, f"details_{cat.lower()}.txt")
        with open(details_path, "w", encoding="utf-8") as f:
            f.write(f"Intersection ({len(inter_cat)}):\n")
            for q in sorted(inter_cat):
                f.write(q + "\n")
            f.write("\nWiki-only:\n")
            for q in sorted(wiki_only_cat):
                f.write(q + "\n")
            f.write("\nWD-only:\n")
            for q in sorted(wd_only_cat):
                f.write(q + "\n")
        print(f"  -> [{cat}] detailed QID lists written to {details_path}")

        # Optional per-bucket Venn diagram
        generate_venn_diagram(wiki_cat, wd_cat, cat)

    # 4) Global & per-country visualisations (beyond Venn)
    print("\nCreating additional visualisations ...")
    plot_global_overlap_bar(wiki_qids, wd_qids)
    plot_per_country_source_bar(stats, categories)
    plot_per_country_overlap_composition(stats, categories)
    plot_per_country_jaccard(stats, categories)
    print("Done. All figures saved in:", OUTPUT_DIR)


if __name__ == "__main__":
    perform_analysis()
